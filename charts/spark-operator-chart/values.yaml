#
# Copyright 2024 The Kubeflow authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default values for spark-operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- String to partially override `spark-operator.fullname` template (will maintain the release name)
nameOverride: ""

# -- String to override release name
fullnameOverride: ""

# -- Common labels to add to the resources
commonLabels: {}

image:
  # -- Image repository
  repository: docker.io/kubeflow/spark-operator
  # -- Image tag, if not set, the chart appVersion will be used.
  tag: ""
  # -- Image pull policy
  pullPolicy: IfNotPresent
  # -- Image pull secrets
  pullSecrets: []
  # - name: <secret-name>

controller:
  # -- Number of replicas of controller, leader election will be enabled if this is greater than 1
  replicaCount: 1

  # -- Set higher levels for more verbose logging
  logLevel: 1

  uiService:
    # -- Enable UI service creation for Spark application
    enable: true

  # -- Ingress URL format.
  # Requires the UI service to be enabled by setting `controller.uiService.enable` to true.
  ingressUrlFormat: ""

  # -- Operator concurrency, higher values might increase memory usage
  workers: 10

  # -- Operator resync interval. Note that the operator will respond to events (e.g. create, update)
  # unrelated to this setting
  resyncInterval: 30

  # -- A comma-separated list of key=value, or key labels to filter resources during watch and list based on the specified labels.
  labelSelectorFilter: ""

  serviceAccount:
    # -- Specifies whether to create a service account for the controller
    create: true
    # -- Optional name for the controller service account
    name: ""
    # -- Optional annotations for the controller service account
    annotations: {}

  rbac:
    # -- Specifies whether to create RBAC resources for the controller
    create: true
    # -- Optional annotations for the controller RBAC resources
    annotations: {}

webhook:
  # -- Specifies whether to enable webhook server
  enable: true

  # -- Number of replicas of webhook server
  replicaCount: 1

  # -- Set higher levels for more verbose logging
  logLevel: 1

  # -- Specifies webhook port
  port: 9443

  # -- Specifies webhook service port name
  portName: webhook

  # -- Specifies how unrecognized errors are handled, allowed values are `Ignore` or `Fail`.
  failurePolicy: Fail

  # -- Specifies the timeout seconds of the webhook, the value must be between 1 and 30.
  timeoutSeconds: 10

  serviceAccount:
    # -- Specifies whether to create a service account for the webhook
    create: true
    # -- Optional name for the webhook service account
    name: ""
    # -- Optional annotations for the webhook service account
    annotations: {}

  rbac:
    # -- Specifies whether to create RBAC resources for the webhook
    create: true
    # -- Optional annotations for the webhook RBAC resources
    annotations: {}

spark:
  # -- List of namespaces where to run spark jobs
  jobNamespaces:
  - default

  serviceAccount:
    # -- Specifies whether to create a service account for spark applications
    create: true
    # -- Optional name for the spark service account
    name: ""
    # -- Optional annotations for the spark service account
    annotations: {}

  rbac:
    # -- Specifies whether to create RBAC resources for spark applications
    create: true
    # -- Optional annotations for the spark application RBAC resources
    annotations: {}

prometheus:
  metrics:
    # -- Specifies whether to enable prometheus metrics scraping
    enable: true
    # -- Metrics port
    port: 10254
    # -- Metrics port name
    portName: metrics
    # -- Metrics serving endpoint
    endpoint: /metrics
    # -- Metric prefix, will be added to all exported metrics
    prefix: ""

  # Prometheus pod monitor for controller pods
  podMonitor:
    # -- Specifies whether to create pod monitor.
    # Note that prometheus metrics should be enabled as well.
    create: false
    # -- Pod monitor labels
    labels: {}
    # -- The label to use to retrieve the job name from
    jobLabel: spark-operator-podmonitor
    # -- Prometheus metrics endpoint properties. `metrics.portName` will be used as a port
    podMetricsEndpoint:
      scheme: http
      interval: 5s

# -- Additional labels to add to the pod
podLabels: {}

# -- Additional annotations to add to the pod
podAnnotations: {}

# -- Pod environment variable sources
envFrom: []

# -- Operator volumeMounts
volumeMounts: []

# -- Sidecar containers
sidecars: []

# - Operator volumes
volumes: []

# -- Node labels for pod assignment
nodeSelector: {}

# -- Affinity for pod assignment
affinity: {}

# -- List of node taints to tolerate
tolerations: []

# -- A priority class to be used for running spark-operator pod.
priorityClassName: ""

# -- Pod resource requests and limits
# Note, that each job submission will spawn a JVM within the Spark Operator Pod using "/usr/local/openjdk-11/bin/java -Xmx128m".
# Kubernetes may kill these Java processes at will to enforce resource limits. When that happens, you will see the following error:
# 'failed to run spark-submit for SparkApplication [...]: signal: killed' - when this happens, you may want to increase memory limits.
resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 300Mi
  # requests:
  #   cpu: 100m
  #   memory: 300Mi

# -- Pod security context
podSecurityContext: {}

# -- Container security context
securityContext: {}

batchScheduler:
  # -- Enable batch scheduler for spark jobs scheduling. If enabled, users can specify batch scheduler name in spark application
  enable: false

resourceQuotaEnforcement:
  # -- Whether to enable the ResourceQuota enforcement for SparkApplication resources.
  # Requires the webhook to be enabled by setting `webhook.enable` to true.
  # Ref: https://github.com/kubeflow/spark-operator/blob/master/docs/user-guide.md#enabling-resource-quota-enforcement.
  enable: false

istio:
  # -- When using `istio`, spark jobs need to run without a sidecar to properly terminate
  enable: false
